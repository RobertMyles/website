---
title: Stan or JAGS for Bayesian ideal-point IRT?
date: '2016-04-13'
categories:
  - R
  - Stan
  - JAGS
  - IRT
  - Ideal Points
  - Political Science
tags: []
description: ''
featured: 'ips.png'
featuredalt: ''
featuredpath: '/images'
linktitle: ''
---



<p>Anybody who has ever tried to run even a moderately-sized Bayesian IRT model in R (for ideal points as in the political science literature, or otherwise) will know that these models can take a <em>long</em> time. It’s not R’s fault: these are usually big models with lots of parameters, and naturally take longer.<sup id="a1"><a href="#fn1">1</a></sup> Not to mention the fact that Bayesian computation is more computationally intense than other methods. Historically (okay, I’m talking about the last twenty years, maybe ‘historically’ is a little strong), the sampling software <a href="http://www.mrc-bsu.cam.ac.uk/software/bugs/">BUGS</a> (<strong>B</strong>ayesian <strong>I</strong>nference <strong>U</strong>sing <strong>G</strong>ibbs <strong>S</strong>ampling) and then <a href="http://mcmc-jags.sourceforge.net/">JAGS</a> were used to run Bayesian models (JAGS is still pretty common, and BUGS too, though not as much). Lately, <a href="http://mc-stan.org/">Stan</a> has been gaining ground, certainly as regards more complex modelling.</p>
<p> </p>
<p>While the reasons for choosing Stan are often put down to speed, when running many types of models there is not actually a large difference, with JAGS actually being faster for some models, according to John Kruschke<sup id="a2"><a href="#fn2">2</a></sup>. Given the lack of a big difference between JAGS/BUGS and Stan, which sampling software should we use for IRT models? Well, first of all, a large part of the literature utilises either JAGS or BUGS, indeed, code is publicly available for many of these models, helping to spread the use of these two modelling languages.<sup id="a3"><a href="#fn3">3</a></sup> For beginners, this is a handy way to learn, and it’s how I learned. Indeed, the language of JAGS/BUGS (I’m just going to use ‘JAGS’ to refer to both from now on) is a bit more intuitive for many people, and given the availability of others’ code, beginning with these models can then be reduced to just tinkering with small details of code that is already written.</p>
<p> </p>
<p>Stan, on the other hand, is newer and has a syntax that is in some ways quite different from JAGS. Variables need to be declared, as does their type (something not many R users are familiar with, I certainly wasn’t). The model code is imperative, not declarative<sup id="a4"><a href="#fn4">4</a></sup>, and there are specific ‘blocks’ to the code. Stan has a different default sampler and is generally argued by its creators to be much faster. Well, in my experience, there is actually no contest. As much as I liked JAGS when I started out, Stan is simply incomparable to JAGS in terms of speed for these models– Stan is much, much faster. I was analysing nominal vote data for the Brazilian Federal Senate<sup id="a5"><a href="#fn5">5</a></sup> (these data have plenty of missing values, which are handled easily in JAGS but have to be deleted out in Stan) and, through the use of the <a href="http://runjags.sourceforge.net/quickjags.html">runjags</a> package (and its <code>autorun</code> option), I discovered that it would take around 28 hours to run my two-dimensional model to reach signs of convergence (or signs of non-convergence, as <a href="pan.oxfordjournals.org/content/16/2/153.full.pdf">Gill</a> puts it). As I was in the middle of writing a PhD thesis with lots of these models to process, that just wasn’t an option. (Regardless, any time I let the model run like this, R crashed or became unresponsive, or the estimates were simply of bad quality.) So I started tinkering with the options in <code>runjags</code>, trying different samplers etc. Then I noticed exactly <em>why</em> JAGS is so slow for these models.</p>
<p> </p>
<p>In order to run a model, JAGS first compiles a Directed Acyclic Graph (DAG) of all the nodes in the model (software such as <a href="http://r-nimble.org/">NIMBLE</a> will let you print out the graph pretty easily). But since we have a <em>latent</em> regression with an <em>unobserved</em> regressor in the equation<sup id="a6"><a href="#fn6">6</a></sup></p>
<p><span class="math display">\[y_{ij} = \beta_j\bf{x_i} - \alpha_j\]</span></p>
<p>then JAGS is <a href="https://sourceforge.net/p/mcmc-jags/discussion/610037/thread/5c9e9026/">unable</a> to build such a DAG. Since it can’t build a DAG, it can’t surmise that there is conjugacy in the model and then exploit that through Gibbs sampling. So JAGS just uses the default Metropolis-Hastings sampler (and given that it is called <strong>J</strong>ust <strong>A</strong>nother <strong>Gibbs</strong> <strong>S</strong>ampler, it kind of misses the point of using JAGS in the first place). This means that all the gains available through Gibbs sampling are simply not available for latent models of this type with JAGS, and hence the sampling process runs <em>very</em> slowly. I’m not sure the literature was ever aware of this fact, either. Many papers and books extoll the virtues of Gibbs sampling (and spend pages and pages deriving the conditional distributions involved) and then show the reader how to do it in JAGS or BUGS (see Simon Jackman’s <a href="http://www.wiley.com/WileyCDA/WileyTitle/productCd-0470011548.html">book</a> for an example)<sup id="a7"><a href="#fn7">7</a></sup>, but unbeknownst to these authors, their JAGS programs are not using Gibbs sampling.</p>
<p> </p>
<p>So that leaves us with Stan. Use it! :smile:</p>
<p> </p>
<p><em>In a future post, I’ll show some examples of IRT ideal-point models in Stan. I have some on my <a href="https://github.com/RobertMyles/Bayesian-Ideal-Point-IRT-Models">Github</a>, and Pablo Barberá also has some nice <a href="https://github.com/pablobarbera/quant3materials/tree/master/bayesian">examples</a> (hat tip: I learned from him, amongst others. Thanks, Pablo!).</em></p>
<p> </p>
<p><em>Update: <a href="https://github.com/duarteguilherme/Quinn-Martin-Replication">Guilherme Jardim Duarte</a> also has some Bayesian IRT examples on his Github, in particular the rather tricky dynamic model of <a href="http://mqscores.berkeley.edu/media/pa02.pdf">Martin &amp; Quinn</a>, have a look.</em></p>
<p> </p>
<p><b id="fn1">1</b> For more on how these models can have <em>tons</em> of parameters, see <a href="https://www.cs.princeton.edu/courses/archive/fall09/cos597A/papers/ClintonJackmanRivers2004.pdf">Clinton, Jackman, and Rivers (2004)</a>: ‘The statistical analysis of roll-call data’, <em>American Political Science Review</em>, Vol. 98, No. 2. <a href="#a1">↩</a></p>
<p><b id="fn2">2</b> <a href="http://doingbayesiandataanalysis.blogspot.com.br/">Kruschke</a> mentions this in his book…not sure where, exactly. <a href="#a2">↩</a></p>
<p><b id="fn3">3</b> See this paper by <a href="https://www.jstatsoft.org/article/view/v036c01/v36c01.pdf">Curtis</a> (pdf downloads automatically) or the book by <a href="https://www.crcpress.com/Analyzing-Spatial-Models-of-Choice-and-Judgment-with-R/Armstrong-II-Bakker-Carroll-Hare-Poole-Rosenthal/9781466517158">Armstrong et. al</a>. <a href="#a3">↩</a></p>
<p><b id="fn4">4</b> See <a href="http://stackoverflow.com/questions/129628/what-is-declarative-programming">here</a> for the difference.<a href="#a4">↩</a></p>
<p><b id="fn5">5</b> You can read about this research <a href="%7B%7B%20site.url%20%7D%7D/assets/Explaining%20the%20Determinants%20of%20Foreign%20Policy%20Voting%20Behaviour%20in%20the%20Brazilian%20Houses%20of%20Legislature.pdf">here</a>.<a href="#a5">↩</a></p>
<p><b id="fn6">6</b> This is the canonical statistical model for Bayesian IRT. The data (<span class="math inline">\(y_{ij}\)</span>) are the votes, in binary form (1 = ‘Yes’; 2 = ‘No’); the <span class="math inline">\(x_i\)</span> are the ideal points of the legislators; and <span class="math inline">\(\beta_j\)</span> and <span class="math inline">\(\alpha_j\)</span> are the <em>discrimination</em> (slope) and <em>difficulty</em> (intercept) parameters, respectively. See the article cited in footnote 1. <a href="#a6">↩</a></p>
<p><b id="fn7">7</b> I don’t mean to denigrate Jackman’s book. It’s highly detailed and thorough, and he deserves a lot of credit for spearheading the use of these Bayesian IRT models in political science. I’ve cited his work numerous times, I’m a fan.<a href="#a7">↩</a> <link rel="image_src" href="http://i.imgur.com/v7y6SVt.png?1" /></p>
